---
title: "Homework"
author: "Jingguo Lan"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# 第1次作业解答

## Question
Produce 3 examples(texts,figures,tables)


## Answer

### Texts
```{r}
a=c("今天","吃饭")
b=c("你","了吗?")
d=paste0(a,b,collapse="")  ## 拼接字符串a,b
d
```


### Figures
```{r}
## 从均值为100，方差为1的正态分布中，随机生成30个数
x <- rnorm(30, mean=100, sd=1)
print(round(x,2))

## 30个随机数的散点图
plot(x,main="散点图")


## 30个随机数的直方图
hist(x, col=rainbow(15), 
     main='正态随机数', xlab='', ylab='频数')

```

### ggplot2
ggplot2包的mpg数据集:(从1999年到2008年38款流行车型的燃油经济性数据)
，234*11的数据规模，记录了制造厂商，型号，类别，驱动程序和耗油量

+ cty 和hwy分别记录城市和高速公路驾驶耗油量
+ cyl:气缸数
+ displ:发动机排量
+ drv:驱动系统：前轮驱动(f)、后轮驱动和四轮驱动(4)
+ class:车辆类型,如双座汽车,suv,小型汽车
+ fl:燃料类型

```{r}
## 用ggplot2画图
library(ggplot2)
data(mpg)  ## 导入mpg数据集

ggplot(data=mpg,mapping = aes(x=cty,y=hwy,color=factor(year)))+
  geom_point()+stat_smooth(method = 'loess')+
  scale_shape_manual(values = c(2,5))+
  labs(y = '每加仑高速公路行驶距离', x = '每加仑城市公路行驶距离',
       title = '汽车油耗与型号', size = '排量', colour = '车型')+
  theme(plot.title = element_text(hjust = 0.5))

```



### Tables


```{r}
library(xtable)
## 显示表格
knitr::kable(head(mpg))
```

# 第2次作业解答

## Question
3.4, 3.11, and 3.20


## Answer

### 3.4 
The Rayleigh density [156, Ch. 18] is
$$
f(x)=\frac{x}{\sigma^{2}} e^{-x^{2} /\left(2 \sigma^{2}\right)}, \quad x \geq 0, \sigma>0 .
$$
Develop an algorithm to generate random samples from a Rayleigh $(\sigma)$ distribution. Generate Rayleigh $(\sigma)$ samples for several choices of $\sigma>0$ and check that the mode of the generated samples is close to the theoretical mode $\sigma$ (check the histogram).

*解:* 

Rayleigh随机变量$X$的分布函数:
$$
F(x)=1-\exp \left(-\frac{x^{2}}{2 \sigma^{2}}\right), x \geq 0
$$
所以$F^{-1}(y)=\sigma \sqrt{-2 \ln (1-y)}$

```{r}
Rayleigh =function(sigma, n){
  for(i in 1:n) {
    U=runif(n)
    V=1-U
    X = sigma * sqrt(-2 * log(V))
  }
  return(X)
}
sigma = 2
n = 1000

hist(Rayleigh(sigma, n),main = "Rayleigh",xlab="")
```

#### 多试几组sigma
```{r,eval=FALSE}
sigma=c(1:9)
n=1000
par(mfrow=c(3,3))
for(i in 1:9){
  title=paste0("sigma=",sigma[i])
  hist(Rayleigh(sigma[i], n),main = title,xlab="")
}
```





### 3.11
Generate a random sample of size 1000 from a normal location mixture. The components of the mixture have $N(0,1)$ and $N(3,1)$ distributions with mixing probabilities $p_{1}$ and $p_{2}=1-p_{1}$. Graph the histogram of the sample with density superimposed, for $p_{1}=0.75 .$ Repeat with different values for $p_{1}$ and observe whether the empirical distribution of the mixture appears to be bimodal. Make a conjecture about the values of $p_{1}$ that produce bimodal mixtures.

 
#### p1取0.75
```{r}
n=1000
X1=rnorm(n,0,1)
X2=rnorm(n,3,1)
u <- runif(n)
p1=as.integer(u < 0.75) 
p2=1-p1
Z=p1*X1+p2*X2
hist(Z,main = "p1=0.75")
```


#### p1从0变化到1
```{r, eval=FALSE}
p=seq(0,1,0.1)
par(mfrow=c(3,4))
for(i in 1:11){
  p1=as.integer(u < p[i]) 
  p2=1-p1
  Z=p1*X1+p2*X2
  title=paste0("p1=",p[i])
  hist(Z,main=title)
}
```


把$p1$的值从0变化到1，发现: 当$p1$在$0.5$附近的时候容易出现双峰.


### 3.20
A compound Poisson process is a stochastic process $\{X(t), t \geq 0\}$ that can be represented as the random sum $X(t)=\sum_{i=1}^{N(t)} Y_{i}, t \geq 0$, where $\{N(t), t \geq 0\}$ is a Poisson process and $Y_{1}, Y_{2}, \ldots$ are id and independent of $\{N(t), t \geq 0\}$. Write a program to simulate a compound Poisson $(\lambda)$-Gamma process $(Y$ has a Gamma distribution). Estimate the mean and the variance of $X(10)$ for several choices of the parameters and compare with the theoretical values. Hint: Show that $E[X(t)]=\lambda t E\left[Y_{1}\right]$ and $\operatorname{Var}(X(t))=\lambda t E\left[Y_{1}^{2}\right]$.



```{r}
Poisson_Gamma=function(n, t, lambda, r, beta) {
  N =rpois(n, lambda * t)
  X=sapply(N, function(N, r, beta) sum(rgamma(N, r,beta)), r,beta)
  return(X)
}

test=function(n, t, lambda, r, beta) {
  x=Poisson_Gamma(n, t, lambda, r,beta)
  ## 样本均值
  sm=mean(x) 
  ## 理论均值
  vm=var(x)
  ## 样本方差
  tm=lambda * t * r/beta
  ## 理论方差
  tv=lambda * t * (1 + r) * r/beta^2
  
  ## 输出结果样式
  cat("r=",r,"beta=",beta,"\n")
  cat("样本均值:", sm, "  ")
  cat("理论均值:", vm, "\n")
  cat("样本方差:", tm, "  ")
  cat("理论方差:", tv, "\n\n")
}

## 参数值
n=1000
lambda_seq=c(1:3)
r_seq=c(1:3)
beta_seq=c(1:3)
t=10

for (lambda in lambda_seq) {
  for (r in r_seq) {
    for (beta in beta_seq) {
      test(n, t, lambda,r,beta)
    }
  }
}

```


# 第3次作业解答

## Question
Exercises 5.4, 5.9, 5.13, and 5.14


## Answer

### 5.4
Write a function to compute a Monte Carlo estimate of the Beta(3, 3) cdf,
and use the function to estimate F(x) for x = 0.1, 0.2, . . ., 0.9. Compare the
estimates with the values returned by the pbeta function in R.

Solution: $Beta(3,3)=\int_0^1 t^2(1-t)^2dt$

```{r}
MCBeta = function(x,n=10000){
  t=runif(n, min = 0, max = x)
  theta.hat=x*mean(t*t*(1-t)*(1-t))/beta(3,3)
  return(theta.hat)
}
x=seq(0.1,0.9,0.1)

MC.Beta=pbeta=rep(0,9)
for(i in 1:9)
{
  MC.Beta[i]=MCBeta(x[i])
  pbeta[i]=pbeta(x[i],3,3)
}
out=rbind(MC.Beta,pbeta)

rownames(out)=c("MC.Beta","pbeta")
colname=NULL
for(i in 9:1){
  name=paste("x=",x[i])
  colname=cbind(name,colname)
}
colnames(out)=colname

out
```


### 5.9
The Rayleigh density $[156,(18.76)]$ is
$$
f(x)=\frac{x}{\sigma^{2}} e^{-x^{2} /\left(2 \sigma^{2}\right)}, \quad x \geq 0, \sigma>0
$$
Implement a function to generate samples from a Rayleigh $(\sigma)$ distribution, using antithetic variables. What is the percent reduction in variance of $\frac{X+X^{\prime}}{2}$ compared with $\frac{X_{1}+X_{2}}{2}$ for independent $X_{1}, X_{2} ?$

```{r}
rayleigh=function(scale, n) {
  rayleigh=antithetic=numeric(n)
  for (i in 1:n) {
    U =runif(n)
    V = 1 - U
    rayleigh = scale * sqrt(-2 * log(U))
    antithetic = scale * sqrt(-2 * log(V))
    out$ray=rayleigh
    out$ant=antithetic
  }
  return(out)
}

scale=2
n=1000
out=rayleigh(scale, n)
var1 = var(out$ray)
var2 =(var(out$ray) + var(out$ant) + 2 * cov(out$ray, out$ant)) / 4
reduction = ((var1 - var2) / var1)
cat("reduction=",100*reduction,"%")
```

### 5.13
Find two importance functions $f_{1}$ and $f_{2}$ that are supported on $(1, \infty)$ and are 'close' to
$$
g(x)=\frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2}, \quad x>1
$$
Which of your two importance functions should produce the smaller variance in estimating
$$
\int_{1}^{\infty} \frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2} d x
$$
by importance sampling? Explain.

Solution: 
Consider $$f_1(x) = e^{-x}, x \in (1,\infty)$$ and $$f_2(x) = \frac{1}{x^2}, x \in (1,\infty)$$

```{r}
x =seq(1,10,0.02)
y = x^2/sqrt(2*pi)* exp((-x^2/2))
y1 = exp(-x)
y2 =1 / (x^2)

gs =c(expression(g(x)==e^{-x^2/2}*x^2/sqrt(2*pi)),expression(f[1](x)==1/(x^2)),expression(f[2](x)==x*e^{(1-x^2)/4}/sqrt(2*pi)))

plot(x, y, type = "l", ylab = "", ylim = c(0,0.5),main='density function')
lines(x, y1, lty = 2,col="red")
lines(x, y2, lty = 3,col="blue")
legend("topright", legend = 0:2,lty = 1:3,col=c("black","red","blue"))

plot(x, y/y1,type = "l",lty = 2, col="red",main = 'ratios')
lines(x, y/y2, lty = 3,col="blue")
legend("topright", legend = 1:2,lty = 2:3,col=c("red","blue"))

```

```{r}
f1 = function(x) { exp(-x) }
f2 = function(x) { (pi * (1 + x^2))^(-1) * (x >= 1) }
g =function(x) {x^2*exp(-x^2/2)/sqrt(2*pi)*(x>1)}

m = 10^7
x1 = rexp(m)
x2 = rcauchy(m)
x2[which(x2 < 1)] = 1 

fg = cbind(g(x1) / f1(x1), g(x2) / f2(x2))

theta.hat = se = numeric(2)
theta.hat =c(mean(fg[,1]), mean(fg[,2]))
se = c(sd(fg[,1]), sd(fg[,2]))
rbind(theta.hat, se)
```



### 5.14
Obtain a Monte Carlo estimate of
$$
\int_{1}^{\infty} \frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2} d x
$$
by importance sampling.


```{r}
g =function(x) {x^2*exp(-x^2/2)/sqrt(2*pi)*(x>1)}
m = 1e4                 
u = runif(m) 
x = 1/(1-u)  
fg = g(x)*x^2
theta.hat = mean(fg)
print(theta.hat)
theta =integrate(g,1,Inf)
theta
```



# 第4次作业解答


## Question
 + Exercises 6.5 and 6.A
 
 + If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. We want to know if the
powers are different at 0.05 level. 

  + What is the corresponding hypothesis test problem? 

  + What test should we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why?

  + Please provide the least necessary information for hypothesis testing


## Answer

### 6.5
Suppose a 95% symmetric t-interval is applied to estimate a mean, but the
sample data are non-normal. Then the probability that the confidence interval
covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment
to estimate the coverage probability of the t-interval for random samples of
$χ^2(2)$ data with sample size n = 20. Compare your t-interval results with the
simulation results in Example 6.4. (The t-interval should be more robust to
departures from normality than the interval for variance.)


```{r}
n <- 20
m <- 1000 
alpha <- .05
UCL <- replicate(m,expr = {
  x <- rchisq(n, df = 2)
  (n-1) * var(x) / qchisq(alpha, df = n-1)
})
cat("coverage probability=", mean(UCL > 4))
```
the 0.95 confidence interval is$\left[\bar{X}-\frac{S}{\sqrt{n}} t_{n-1}(0.975), \bar{X}+\frac{S}{\sqrt{n}} t_{n-1}(0.975)\right]$

```{r}
n <- 20 
alpha <- .05
x <- rchisq(n, df = 2)
prob <- replicate(m,expr = {
  x <- rchisq(n, df = 2)
  abs(mean(x)-2) < sd(x) * qt(alpha/2, df = n-1,lower.tail = FALSE)/sqrt(n)
})
cat("coverage probability=", mean(prob))
```
如果样本不是正态分布的，那么置信区间覆盖方差的概率就不一定接近0.95，但t-interval表现更稳健。



### 6.A

Use Monte Carlo simulation to investigate whether the empirical Type I error rate of the t-test is approximately equal to the nominal significance level$\alpha$, when the sampled population is non-normal. The t-test is robust to mild departures from normality. 
Discuss the simulation results for the cases where the sampled population is 
(i) $X^2(1)$, (ii) Uniform$(0,2)$, and (iii) Exponential(rate=1).

In each case, test 
$H_0 : \mu= \mu_0 ~~vs~~ H_0 : \mu \neq \mu_0$, where $\mu_0$ is the mean of $X^2(1)$, Uniform$(0,2)$, and Exponential(1), respectively.
```{r}
num<-c(50,100,200,500,1000) # 不同样本量
m<-10000

er<-NULL
for (n in num){
  cv<-qt(0.975,n-1)
  er1<-mean(sapply(1:m,FUN = function(o){
  x<-rchisq(n,1)
  m<-mean(x)
  se<-sqrt(var(x))
  abs((m-1)*sqrt(n)/se)>=cv
  })) 
  er2<-mean(sapply(1:m,FUN = function(o){
  x<-runif(n,0,2)
  m<-mean(x)
  se<-sqrt(var(x))
  abs((m-1)*sqrt(n)/se)>=cv
  }))
  er3<-mean(sapply(1:m,FUN = function(o){
  x<-rexp(n,1)
  m<-mean(x)
  se<-sqrt(var(x))
  abs((m-1)*sqrt(n)/se)>=cv
  }))
  er<-cbind(er,c(er1,er2,er3))
}
colnames(er)<-num
rownames(er)<-c("Chi(1)","U(0,2)","exp(1)")
knitr::kable(er)
```





### Discussion

If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, $0.651$ for one method and $0.676$ for another method. We want to know if the powers are different at $0.05$ level.

+ What is the corresponding hypothesis test problem?

+ What test should we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why?

+ Please provide the least necessary information for hypothesis testing.

**Solution.**

**1.** $H_0: \text{The two methods have the same power.} \leftrightarrow H_1: \text{The two methods have different powers.}$

**2.** McNemar test. Because it is equivalent to test whether the acceptance rates of the two methods are the same. Also, a contingency table can be naturally constructed as in **3**. 

**3.** For instance, consider the following contingency table. 
```{r}
mat <-
  matrix(c(6510, 3490, 10000, 6760, 3240, 10000, 13270, 6730, 20000), 3, 3,
         dimnames = list(
           c("Rejected", "Accepted", "total"),
           c("method A", "method B", "total")
         ))
mat
```
The test statistic:
$$\chi^2 = \sum_{i,j=1}^2 \frac{(n_{ij}-n_{i+} n_{+j}/n)^2}{n_{i+}n_{+j}/n} \rightarrow \chi^2_1.$$
Note that $\chi^2 = 13.9966$ and the p-value is $P(\chi^2_1 > \chi^2) = 0.0001831415 < 0.05$. Therefore, we reject the null hypothesis $H_0$, that is, the powers are different at $0.05$ level.


# 第5次作业解答

## Question
Exercise 6.C

Repeat Examples $6.8$ and $6.10$ for Mardia's multivariate skewness test. Mardia [187] proposed tests of multivariate normality based on multivariate generalizations of skewness and kurtosis. If $X$ and $Y$ are iid, the multivariate population skewness $\beta_{1, d}$ is defined by Mardia as
$$
\beta_{1, d}=E\left[(X-\mu)^{T} \Sigma^{-1}(Y-\mu)\right]^{3}
$$
Under normality, $\beta_{1, d}=0 .$ The multivariate skewness statistic is
$$
b_{1, d}=\frac{1}{n^{2}} \sum_{i, j=1}^{n}\left(\left(X_{i}-\bar{X}\right)^{T} \widehat{\Sigma}^{-1}\left(X_{j}-\bar{X}\right)\right)^{3}
$$
where $\hat{\Sigma}$ is the maximum likelihood estimator of covariance. Large values of $b_{1, d}$ are significant. The asymptotic distribution of $n b_{1, d} / 6$ is chisquared with $d(d+1)(d+2) / 6$ degrees of freedom.


## Answer

### 多元正态性的偏度检验(6.8)

假设为
$$H_0:\beta_{1,d}=0 \leftrightarrow H_1:\beta_{1,d}\neq 0$$
当多元总体为正态时，$\frac{nb_{1,d}}{6}$的渐进分布为$\chi_{d(d+1)(d+2)/6}^2$，对大的$|\beta_{1,d}|$值拒绝原假设。\
对大小为$n=10,20,30,50,100,500$的样本，估计基于$\frac{nb_{1,d}}{6}$的渐进分布的多元正态性偏度检验在显著水平$\alpha=0.05$下的第一类错误率，在正态极限分布下计算临界值向量并存储到$b_0$中。并给出*mul.sk()*函数用来计算样本多元偏度统计量。

```{r,eval=FALSE}
nn <- c(10,20,30,50,100,500)  # 样本容量
alpha <- 0.05                 # 显著性水平
d <- 2                        # 随机变量的维数
b0 <- qchisq(1-alpha,df=d*(d+1)*(d+2)/6)*6/nn  # 每种样本容量临界值向量

# 计算多元样本偏度统计量
mul.sk <- function(x){
  n <- nrow(x) # 样本个数
  xbar <- colMeans(x) 
  sigma.hat <- (n-1)/n*cov(x) # MLE估计
  
  b <- 0
  for(i in 1:nrow(x)){
    for(j in 1:nrow(x)){
      b <- b+((x[i,]-xbar)%*%solve(sigma.hat)%*%(x[j,]-xbar))^3
    }
  }
  return(b/(n^2))
}

# 计算第一类错误的经验估计
library(mvtnorm)
set.seed(200)
p.reject <- vector(mode = "numeric",length = length(nn)) # 保存模拟结果

m <- 1000

for(i in 1:length(nn)){
  mul.sktests <- vector(mode = "numeric",length = m)
  for(j in 1:m){
    data <- rmvnorm(nn[i],mean = rep(0,d))
    mul.sktests[j] <- as.integer(mul.sk(data)>b0[i])
  }
  p.reject[i] <- mean(mul.sktests)
}
p.reject
```

模拟结果为第一类错误的经验估计，总结如下:
```{r,eval=FALSE}
summ <- rbind(nn,p.reject)
rownames(summ) <- c("n","estimate")
knitr::kable(summ)
```

模拟的结果说明渐进卡方分布对大小$n\leq 50$的小样本并不合适，需要进一步求方差的精确值。


### 多元正态性偏度检验的功效(6.10)

类似例6.10，针对污染正态备择假设，通过模拟估计多元正态性偏度检验的功效，污染正态分布表示如下：
$$(1-\epsilon)N(0,I_d)+\epsilon N(0,100I_d),0 \leq \epsilon \leq 1$$
对一列以$\epsilon$为指标的备择假设估计其多元偏度检验的功效，并绘制检验功效的功效函数。显著性水平$\alpha=0.1$，样本大小为$n=30$。

```{r,eval=FALSE}
alpha <- 0.1
n <- 30      # 样本大小
m <- 2000    # 重复次数
epsilon <- c(seq(0,0.15,0.01),seq(0.15,1,0.05))
N <- length(epsilon)
power <- vector(mode = "numeric",length = N)
b0 <- qchisq(1-alpha,df=d*(d+1)*(d+2)/6)*6/n  #临界值

# 对这列epsilon分别求power
for(j in 1:N){
  e <- epsilon[j]
  mul.sktests <- numeric(m)
  for(i in 1:m){
    # 生成混合分布
    u <- sample(c(1,0),size = n,replace = T,prob = c(1-e,e))
    data1 <- rmvnorm(n,sigma = diag(1,d))
    data2 <- rmvnorm(n,sigma = diag(100,d))
    data <- u*data1+(1-u)*data2
    mul.sktests[i] <- as.integer(mul.sk(data)>b0)
  }
  power[j] <- mean(mul.sktests)
}

# 绘制功效函数
plot(epsilon,power,type="b",xlab=bquote(epsilon),ylim=c(0,1))
abline(h=0.1,lty=3,col="lightblue")
se <- sqrt(power*(1-power)/m)  # 绘制标准误差
lines(epsilon,power-se,lty=3)
lines(epsilon,power+se,lty=3)
```

从图中可以看出，功效函数在两个端点$\epsilon=0$和$\epsilon=1$处与$\alpha=0.1$对应的水平线相较，对于$0<\epsilon<1$，经验功效函数要大于0.1，且在0.15左右达到最高。


# 第7次作业解答

## Question

Exercise 8.2 (page 242, Statistical Computating with R).

Design experiments for evaluating the performance of the NN,
energy, and ball methods in various situations.

+ Unequal variances and equal expectations

+ Unequal variances and unequal expectations

+ Non-normal distributions: t distribution with 1 df (heavy-tailed
distribution), bimodel distribution (mixture of two normal
distributions)

+ Unbalanced samples (say, 1 case versus 10 controls)

+ Note: The parameters should be chosen such that the powers
are distinguishable (say, range from 0.3 to 0.8).

## Answer

### 8.2 
Implement the bivariate Spearman rank correlation test for independence
[255] as a permutation test. The Spearman rank correlation test statistic can
be obtained from function cor with method = "spearman". Compare the
achieved significance level of the permutation test with the p-value reported
by cor.test on the same samples.

#### Spearman rank correlation test

```{r}
Spearman_rank_test <- function(x, y, B = 1e4){
  t0 = cor(x,y,method = "spearman")
  perm = numeric(B)
  z = c(x,y)
  for(i in 1:B){
    samp = sample(z)
    perm[i] = cor(samp[1:length(x)], samp[(length(x)+1):length(z)], method = "spearman")
  }
  p_value = mean(abs(perm)>=abs(t0))
  return(list(statistic = t0, 'p.value' = p_value))
}
```

#### Independent case

We generate 50 samples from $N(0,1)$ for $X$ and $Y$, respectively. We expect the p-value of the test is very large so that the null hypothesis would not be rejected.

```{r}
n = 50
x = rnorm(n, 0, 1)
y = rnorm(n, 0, 1)
```


```{r}
Spearman_rank_test(x, y)
cor.test(x, y, method = "spearman")
```

From the output of `Spearman_rank_test()` and `cor.test`, we can see the p-value of these two methods very close and both of them will not reject the null hypothesis, which mean two samples are independent.


#### Dependent case

Consider the data $X\sim N(0,1)$, $Y_i = X_i + \varepsilon_i$, where $\varepsilon_i\sim N(0,1)$. $X$ and $Y$ are highly dependent, we expect the p-value of the test is very small.


```{r}
n = 50
x = rnorm(n, 0, 1)
y = x + rnorm(n, 0, 1)
```


```{r}
Spearman_rank_test(x, y)
cor.test(x, y, method = "spearman")
```


From the output of `Spearman_rank_test()` and `cor.test`, we can see p-value of two methods are 0, which means we would reject the null hypothesis, i.e. two samples are not independent.


### Discussion

Design experiments for evaluating the performance of the NN,
energy, and ball methods in various situations.

+ Unequal variances and equal expectations

+ Unequal variances and unequal expectations

+ Non-normal distributions: t distribution with 1 df (heavy-tailed
distribution), bimodel distribution (mixture of two normal
distributions)

+ Unbalanced samples (say, 1 case versus 10 controls)

+ Note: The parameters should be chosen such that the powers
are distinguishable (say, range from 0.3 to 0.8).

#### Unequal variances and equal expectations

```{r,eval=FALSE}
library(bootstrap)
library(boot)
library(RANN)
library(energy)
library(Ball)

Tn <- function(z, ix, size, k = 3){
  n1 = size[1]
  n2 = size[2]
  n = n1+n2
  if(is.vector(z)) 
    z = data.frame(z,0)
  z = z[ix,]
  NN = nn2(data=z, k = k+1)
  b1 = NN$nn.idx[1:n1,-1]
  b2 = NN$nn.idx[(n1+1):n,-1]
  i1 = sum(b1<n1+0.5)
  i2 = sum(b2>n1+0.5)
  (i1+i2)/(k+n)
}
eqdist.nn <- function(z, size, k, R = R){
  res = boot(data = z, statistic = Tn, R = R, sim = "permutation", size = size, k = k)
  stat = c(res$t0, res$t)
  p_val = mean(stat>stat[1])
  return(p_val)
}
p_val <- function(x,y, R = 999, k = 3){
  x = as.matrix(x)
  y = as.matrix(y)
  n1 = nrow(x)
  n2 = nrow(y)
  N = c(n1,n2)
  z = rbind(x,y)
  p_nn = eqdist.nn(z, size = N, k, R = R)
  p_energy = eqdist.etest(z,sizes=N,R = R)$p.value
  p_ball = bd.test(x=x,y=y,num.permutations = R)$p.value
  names(p_ball) = "ball"
  return(c(NN = p_nn, energy = p_energy, p_ball))
}
```


#### Unequal variances and equal expectations

Generate $X_1,\cdots,X_{50}$ from $N(0,1)$, $Y_1,\cdots,Y_{50}$ from $N(0,1.7^2)$, $X$ and $Y$ have equal mean and unequal variance.

```{r,eval=FALSE}
res = replicate(200, expr = {
  x = rnorm(50)
  y = rnorm(50, 0, 1.7)
  p_val(x, y)
})
alpha = 0.05
apply(res, 1, function(x)mean(x<alpha))
```


## Unequal variances and unequal expectations


Generate $X_1,\cdots,X_{40}$ from $N(1,1)$, $Y_1,\cdots,Y_{40}$ from $N(0.6,2^2)$, $X$ and $Y$ have unequal mean and unequal variance.

```{r,eval=FALSE}
res = replicate(200, expr = {
  x = rnorm(40, 1, 1)
  y = rnorm(40, 0.6, 2)
  p_val(x, y)
})
alpha = 0.05
apply(res, 1, function(x)mean(x<alpha))
```



#### Non-normal distributions

Generate $X_1,\cdots, X_{40}$ from unifrom distribution U(-3,3), generate $Y_1,\cdots, Y_{60}$ from 

```{r,eval=FALSE}
res = replicate(200, expr = {
  x = runif(40, -3, 3)
  y = rt(60, 5)
  p_val(x, y)
})
alpha = 0.05
apply(res, 1, function(x) mean(x<alpha))
```


#### Unbalanced samples


```{r,eval=FALSE}
res = replicate(200, expr = {
  x = runif(30, -3, 3)
  y = rt(300, 5)
  p_val(x, y)
})
alpha = 0.05
apply(res, 1, function(x) mean(x<alpha))
```


# 第8次作业解答

## Question

+ Exercies 9.3 and 9.8 (pages 277-278, Statistical Computating
with R).

+ For each of the above exercise, use the Gelman-Rubin method
to monitor convergence of the chain, and run the chain until it
converges approximately to the target distribution according to
$\hat R< 1.2 $



## Answer

### 9.3

Use the Metropolis-Hastings sampler to generate random variables from a standard Cauchy distribution. Discard the first 1000 of the chain, and compare the deciles of the generated observations with the deciles of the standard Cauchy distribution (see qcauchy or qt with df=1). Recall that a Cauchy $(\theta, \eta)$ distribution has density function
$$
f(x)=\frac{1}{\theta \pi\left(1+[(x-\eta) / \theta]^{2}\right)}, \quad-\infty<x<\infty, \theta>0
$$
The standard Cauchy has the Cauchy $(\theta=1, \eta=0)$ density. (Note that the standard Cauchy density is equal to the Student $\mathrm{t}$ density with one degree of freedom.)

```{r}
set.seed(123)
## standard Cauchy 
f <- function(x) {
  return(1/(pi*(1+x^2)))
}

m <- 10000
x <- numeric(m)
x[1] <- rnorm(1)
k <- 0
u <- runif(m)
for (i in 2:m) {
  xt <- x[i-1]
  y <- rnorm(1, mean = xt)
  num <- f(y) * dnorm(xt, mean = y)
  den <- f(xt) * dnorm(y, mean = xt)
  if (u[i] <= num/den) x[i] <- y else {
    x[i] <- xt
    k <- k+1 #y is rejected
  }
}


## MC过程图
plot(1:m, x, type="l", main="", ylab="x")
b <- 1000
y <- x[b:m]
a <- ppoints(1000)
Qc <- qcauchy(a) 
Q <- quantile(x, a)


## QQ图
qqplot(Qc, Q, main="",xlim=c(-5,5),ylim=c(-5,5),xlab="Standard Cauchy Quantiles", ylab="Sample Quantiles")

## 直方图
hist(y, breaks="scott", main="", xlab="", freq=FALSE)
lines(Qc, f(Qc))

```


### 9.8

This example appears in [40]. Consider the bivariate density
$$
f(x, y) \propto\left(\begin{array}{l}
n \\
x
\end{array}\right) y^{x+a-1}(1-y)^{n-x+b-1}, \quad x=0,1, \ldots, n, 0 \leq y \leq 1
$$
It can be shown (see e.g. $[23]$ ) that for fixed $a, b, n$, the conditional distributions are $\operatorname{Binomial}(n, y)$ and $\operatorname{Beta}(x+a, n-x+b)$. Use the Gibbs sampler to generate a chain with target joint density $f(x, y)$.

```{r}
set.seed(2)

N <- 5000 #length of chain
burn <- 1000 #burn-in length
a <- 2
b <- 3
n <- 1000
X <- matrix(0, N, 2) #the chain, a bivariate sample

#initialize
x0=2
y0=0.5
###### generate the chain #####
X[1, ] <- c(x0,y0) #initialize
for (i in 2:N) {
  y <- X[i-1, 2]
  X[i, 1] <- rbinom(1,size=n,prob = y)
  x <- X[i, 1]
  X[i, 2] <- rbeta(1,x+a,n-x+b)
}
lab <- burn + 1
out <- X[lab:N, ]
plot(X[,1],X[,2],xlab = "x",ylab = "y")
```

```{r}
cat("协方差矩阵\n")
cov(out) ## 协方差矩阵

cat("相关系数矩阵\n")
cor(out) ## 相关系数矩阵
plot(out, main="", cex=.5, xlab=bquote(X[1]),
ylab=bquote(X[2]), ylim=range(out[,2]))  ## 分布图
```




### Discussion

For each of the above exercise, use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to $\hat R<1.2$

解答：

```{r}
# 计算Gelman-Rubin statistic的函数
Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)

  psi.means <- rowMeans(psi)     #row means
  B <- n * var(psi.means)        #between variance est.
  psi.w <- apply(psi, 1, "var")  #within variances
  W <- mean(psi.w)               #within est.
  v.hat <- W*(n-1)/n + (B/n)     #upper variance est.
  r.hat <- v.hat / W             #G-R statistic
  return(r.hat)
        }
```

#### 9.3

先按照9.3题写出构造标准柯西分布的Metropolis chain的函数
```{r}
# 生成标准柯西分布的Metropolis chain
# 提议函数仍取9.3中使用的对称正态分布 N(0,X[t]^2)
# X1为初始值
Standard_Cauchy_Chain <- function(N, X1){
  X <- numeric(N)
  X[1] <- X1    #初始值
  for(i in 2:N){
    Xt <- X[i-1]
    Y <- rnorm(1,0,abs(Xt))
    r <- dt(Y,1)*dnorm(Xt,0,abs(Y))/dt(Xt,1)/dnorm(Y,0,abs(Xt))
    U <- runif(1)
    if(r > 1) r <- 1
    if(U <= r) X[i] <- Y
    else X[i] <- Xt
  }
  return(X)
}
```


接下来不妨考虑生成4条上述Metropolis chain，每条样本量N=8000。
```{r}
k <- 4      
N <- 8000
b <- 1000     #burn-in length
X1 <- c(0.1,0.2,0.1,0.2)    #初始值

# 生成4条样本
set.seed(12345)
X <- matrix(0, nrow = k, ncol = N)
for(i in 1:k){
  X[i,] <- Standard_Cauchy_Chain(N, X1[i])
}

# compute diagnostic statistics
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
  psi[i,] <- psi[i,] / (1:ncol(psi))
print(Gelman.Rubin(psi))
```


```{r}
# 四条样本的psi
for (i in 1:k)
  if(i==1){
    plot((b+1):N,psi[i, (b+1):N],ylim=c(-1,1), type="l",
         xlab='Index', ylab=bquote(phi))
  }else{
      lines(psi[i, (b+1):N], col=i)
  }
par(mfrow=c(1,1)) 
```

实际上发现四条样本的psi图并没有呈现逼近同一分布的结果，这可能是因为Cauchy分布的期望和方差不均存在，进而导致的估计不稳定性，下面再画出$\hat R$统计量v.s.样本量N的图。

```{r}
par(mfrow=c(1,1)) 
#plot the sequence of R-hat statistics
rhat <- rep(0, N)
for (j in (b+1):N)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):N], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)
```

$\hat R$ 大概在样本为1000时达到收敛。


#### 9.8 

先按照9.8题写出题中二元随机变量的Gibbs sampler，这里不妨取a=b=1。 
```{r}
# 生成二元随机变量的Gibbs sampler
# X1为初始值
Bivariate.Gibbs <- function(N, X1){
  a <- b <- 1
  X <- matrix(0, N, 2)
  X[1,] <- X1    #初始值
  for(i in 2:N){
    X2 <-  X[i-1, 2]
    X[i,1] <- rbinom(1,25,X2)
    X1 <- X[i,1]
    X[i,2] <- rbeta(1,X1+a,25-X1+b)
  }
  return(X)
}
```


不妨还是考虑生成4条样本，每条样本量N=8000.
```{r}
k <- 4          
N <- 8000 
b <- 1000    #burn-in length
X1 <- cbind(c(2,7,10,15),runif(4)) #初始值

#生成4条样本，每个第一维的放在X中，第二维的放在Y中
set.seed(12345)
X <- matrix(0, nrow=k, ncol=N)
Y <- matrix(0, nrow=k, ncol=N)
for (i in 1:k){
  BG <- Bivariate.Gibbs(N, X1[i,])
  X[i, ] <- BG[,1]
  Y[i, ] <- BG[,2]
}
```


下面分别在每一个维度上考虑利用Gelman-Rubin method考虑样本的收敛情况。
```{r}
# 先考虑第一维样本X

#compute diagnostic statistics
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
  psi[i,] <- psi[i,] / (1:ncol(psi))

#plot the sequence of R-hat statistics
rhat <- rep(0, N)
for (j in (b+1):N)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):N], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)
```


```{r}
# 再考虑第二维样本Y

#compute diagnostic statistics
psi <- t(apply(Y, 1, cumsum))
for (i in 1:nrow(psi))
  psi[i,] <- psi[i,] / (1:ncol(psi))

#plot the sequence of R-hat statistics
rhat <- rep(0, N)
for (j in (b+1):N)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):N], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)
```

综合考虑两个维度的$\hat R$统计量，大约在样本为4000时可以达到收敛。


# 第9次作业解答

## Question

+  Exercises 11.3 and 11.5 (pages 353-354, Statistical Computing with R).
+  Suppose $T_{1}, \ldots, T_{n}$ are i.i.d. samples drawn from the exponential distribution with expectation $\lambda$. Those values greater than $\tau$ are not observed due to right censorship, so that the observed values are $Y_{i}=T_{i} I\left(T_{i} \leq \tau\right)+\tau I\left(T_{i}>\tau\right), i=1, \ldots, n$. Suppose $\tau=1$ and the observed $Y_{i}$ values are as follows:$$0.54,0.48,0.33,0.43,1.00,1.00,0.91,1.00,0.21,0.85$$
Use the E-M algorithm to estimate $\lambda$, compare your result with the observed data MLE (note: $Y_{i}$ follows a mixture distribution).


## Answer

### 11.3

(a) Write a function to compute the $k^{t h}$ term in
$$
\sum_{k=0}^{\infty} \frac{(-1)^{k}}{k ! 2^{k}} \frac{\|a\|^{2 k+2}}{(2 k+1)(2 k+2)} \frac{\Gamma\left(\frac{d+1}{2}\right) \Gamma\left(k+\frac{3}{2}\right)}{\Gamma\left(k+\frac{d}{2}+1\right)}
$$
where $d \geq 1$ is an integer, $a$ is a vector in $\mathbb{R}^{d}$, and $\|\cdot\|$ denotes the Euclidean norm. Perform the arithmetic so that the coefficients can be computed for (almost) arbitrarily large $k$ and $d$. (This sum converges for all $a \in \mathbb{R}^{d}$ ).

(b) Modify the function so that it computes and returns the sum.

(c) Evaluate the sum when $a=(1,2)^{T}$.

解:

(a)
```{r}
thek <- function(k, a, d){
  (-1)^k/exp(lgamma(k+1)+k*log(2)) * exp((k+1)*log(sum(a^2))-log(2*k+1)-log(2*k+2)) * exp(lgamma((d+1)/2)+lgamma(k+1.5)-lgamma(k+d/2+1))#用到了gamma函数和阶乘的恒等式
}
```
(b)
```{r}
sumk <- function(a, d){
  k <- 0
  s <- 0
  while(abs(thek(k, a, d))>1e-5){#tolerance
    s <- s+thek(k, a, d)
    k <- k+1
  }
  return(s)
}
```
(c)
```{r}
a <- c(1,2)
d <- length(a)
s <- sumk(a,d)
paste("The sum =", s)
```

### 11.5
 
Write a function to solve the equation

$$
\begin{gathered}
\frac{2 \Gamma\left(\frac{k}{2}\right)}{\sqrt{\pi(k-1)} \Gamma\left(\frac{k-1}{2}\right)} \int_{0}^{c_{k-1}}\left(1+\frac{u^{2}}{k-1}\right)^{-k / 2} d u \\
=\frac{2 \Gamma\left(\frac{k+1}{2}\right)}{\sqrt{\pi k} \Gamma\left(\frac{k}{2}\right)} \int_{0}^{c_{k}}\left(1+\frac{u^{2}}{k}\right)^{-(k+1) / 2} d u
\end{gathered}
$$

for $a$, where

$$
c_{k}=\sqrt{\frac{a^{2} k}{k+1-a^{2}}}
$$

Compare the solutions with the points $A(k)$ in Exercise 11.4.


解:

```{r}
k <- c(4:25, 100, 500, 1000)
###11.5
beijif <- function(u, kf){
  (1+u^2/kf)^(-(kf+1)/2)
}
g <- function(a, kg){
  ckl <- sqrt(a^2*(kg-1)/(kg-a^2))
  LHS <- 2/sqrt(pi*(kg-1)) * exp(lgamma(kg/2)-lgamma((kg-1)/2)) * integrate(beijif, lower = 0, upper = ckl, kf=kg-1)$value
  ckr <- sqrt(a^2*kg/(kg+1-a^2))
  RHS <-2/sqrt(pi*kg) * exp(lgamma((kg+1)/2)-lgamma(kg/2)) * integrate(beijif, lower = 0, upper = ckr, kf=kg)$value
  LHS-RHS
}

solution5 <- numeric(length(k))
for (i in 1:length(k)) {
  solution5[i] <- uniroot(g, c(1,2), kg=k[i])$root
}

###11.4
h <- function (a,kh) {
  (1-pt(sqrt(a^2*(kh-1) / (kh-a^2)), df=kh-1)) - (1-pt(sqrt(a^2*kh / (kh+1-a^2)), df=kh))
}

solution4 <- numeric(length(k))
for (i in 1:length(k)) {
  solution4[i] <- uniroot(h, c(1,2), kh=k[i])$root
}

###Compare
print(cbind(k=k, exercice4=solution4, exercice4=solution5))
```

两种方法结果完全一致。


### Discussion
Suppose $T_{1}, \ldots, T_{n}$ are i.i.d. samples drawn from the exponential distribution with expectation $\lambda$. Those values greater than $\tau$ are not observed due to right censorship, so that the observed values are $Y_{i}=T_{i} I\left(T_{i} \leq \tau\right)+\tau I\left(T_{i}>\tau\right), i=1, \ldots, n$. Suppose $\tau=1$ and the observed $Y_{i}$ values are as follows:$$0.54,0.48,0.33,0.43,1.00,1.00,0.91,1.00,0.21,0.85$$
Use the E-M algorithm to estimate $\lambda$, compare your result with the observed data MLE (note: $Y_{i}$ follows a mixture distribution).


解:

Observed data likelihood:
$$L=\Pi_{i=1}^n\left(\frac{1}{\lambda} e^{-\frac{1}{\lambda} x_{i}}\right)^{k_{i}} \cdot\left(e^{-\frac{1}{\lambda} \tau}\right)^{1-k_{i}}$$
$$\log L=n_{1} \cdot \log \frac{1}{\lambda}+\sum_{x_{i} \leq \tau}\left(-\frac{1}{\lambda} x_{i}\right)+n_{2} \cdot-\frac{1}{\lambda} \tau$$
$$\frac{\partial \log L}{\partial\lambda}=n_{1} \cdot \left(-\frac{1}{\lambda}\right)+ \left(\sum_{x_{i} \leq \tau} \frac{1}{\lambda^2}x_{i}\right)+n_{2}\cdot\frac{1}{\lambda^2}  \tau$$
$$\hat{\lambda}_{MLE}=\frac{\sum_{x_{i} \leq \tau} x_{i}+n_{2} \tau}{n_{1}}$$
Complete data likelihood:
$$L=\Pi_{i=1}^n \frac{1}{\lambda} e^{-\frac{1}{\lambda} x_{i}}$$ 
$$\log L= n \cdot \log \frac{1}{\lambda}-\frac{1}{\lambda} \cdot  \Sigma_{x_{i} \leq \tau} x_{i}-\frac{1}{\lambda}  \Sigma_{x_{i} > \tau} x_{i}$$ 
E-step:
$$\mathbb{E}\log L=n \cdot \log \frac{1}{\lambda}-\frac{1}{\lambda} \cdot \Sigma_{x_{i} \leq \tau} x_{i}- \frac{1}{\lambda}n_{2}\left(\tau+\lambda_{0}\right)$$
$$\frac{\partial \mathbb{E} \log L}{\partial \lambda}=n\cdot \left(-\frac{1}{\lambda}\right)+\frac{1}{\lambda^2}\cdot\Sigma_{x_{i} \leq \tau} x_{i}+\frac{1}{\lambda^2}\cdot n_{2}\left(\tau+\lambda_{0}\right)=0$$
M-step:
$$\lambda_1=\frac{\Sigma_{x_{i} \leq \tau} x_{i}+n_{2}\left(\tau+\lambda_{0}\right)}{n}$$
$$\hat{\lambda}_{EM}=\frac{\sum_{x_{i} \leq \tau} x_{i}+n_{2} \tau}{n_{1}}$$

```{r}
data <- c(0.54,0.48,0.33,0.43,1.00,1.00,0.91,1.00,0.21,0.85)
tau <- 1
n <- length(data)
n1 <- sum(data<tau)
n2 <- n-n1
lam0 <- 0
lam1 <- 1#初始值


i <- 1
while (abs(lam0-lam1)>1e-10) {
  lam0 <- lam1
  # E step
  E <- function(lam) n*log(1/lam)-1/lam*sum(data[data<tau])-n2/lam*(tau+lam0)
  # M step
  lam1 <- optimize(E, lower = 0, upper = 2, maximum = TRUE)$maximum
}

# MLE 
# lam <- 1


lik <- function(lam){
  lik1 <- sapply(data[data<tau], function(x) {
    dexp(x,rate=1/lam)
  })
  lik2 <- sapply(data[data==tau],function(x){
    1-pexp(tau,rate = 1/lam)
  })
  prod(c(lik1,lik2))
}
MLE <- optimize(lik, lower = 0, upper = 2, maximum = TRUE)$maximum
print(cbind(EM=lam1, MLE))
```

结果相差很小.


# 第10次作业解答

## Question

+ Exercises 1 and 5 (page 204, Advanced R)
+ Excecises 1 and 7 (page 214, Advanced R)


## Answer

### P204-1

Why are the following two invocations of `lapply()` equivalent?

```{r results='hide'}
trims <- c(0, 0.1, 0.2, 0.5)
x <- rcauchy(100)
lapply(trims, function(trim) mean(x, trim = trim))
lapply(trims, mean, x = x)
```
答:
mean(x,trim)实际上有两个参数x和trim

+ 第一种表达是构造了新的function,给定了x的值，只剩下一个参数trim, lapply可以直接对trim的每个取值调用新的函数求mean
+ 第二种表达实际上把mean的参数x直接传到lapply里了, 也是对trim每个取值求mean


### P204-5

For each model in the previous two exercises, extract R2 using
the function below.

#### Ex1
```{r}
formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)
lapply(formulas,lm,data=mtcars)
```



#### Ex2
```{r}
bootstraps <- lapply(1:10, function(i) {
  rows <- sample(1:nrow(mtcars), rep = TRUE)
  mtcars[rows, ]
})

lapply(bootstraps,function(t) lm(mpg~disp,data=t))
```


```{r}
rsq <- function(mod) summary(mod)$r.squared

## Ex1
cat("这是Ex1 4个模型的R-square\n")
lapply(lapply(formulas,lm,data=mtcars), rsq)

## Ex2
cat("这是Ex2 10个模型的R-square\n")
lapply(lapply(bootstraps,function(t) lm(mpg~disp,data=t)), rsq)
```


### P214-1

Use `vapply()` to:

  + a) Compute the standard deviation of every column in a numeric data frame.
  + b) Compute the standard deviation of every numeric column in a mixed data frame. 
  (Hint: you’ll need to use `vapply()` twice.)


#### (a)
```{r}
df= data.frame(a = 1:4, b= 5:8, c =9:12)

## 每一列取标准差
vapply(as.list(df),sd,numeric(1))
```
#### (b)
```{r}
df2=data.frame(a = 1:4, b=c("x","y","z","s"), c =9:12)
vapply(df2[vapply(df2, is.numeric, logical(1))], sd, numeric(1))
```


### P214-7

Implement `mcsapply()`, a multicore version of `sapply()`. Can
you implement mcvapply(), a parallel version of `vapply()`?
Why or why not?

#### mcsaplly()

下面分配四个核来构造mcsapply函数

```{r}
library(parallel)
mcsapply <- function(n, func){
  core <- makeCluster(4)    # 使用4个核
  res <- parSapply(core, n, func)   # 并行计算，n为次数，func为函数
  stopCluster(core)         # 关闭核
}
```

然后不妨尝试还是考虑前面习题中提及的计算置换数据的$R^2$

```{r, eval=FALSE}
R2 <- function(i){
  index <- sample(1:nrow(mtcars), rep = TRUE)
  m <- lm(mpg ~ disp, data = mtcars[index,])
  return(summary(m)$r.squared)
}

# 使用sapply函数进行10次
system.time(sapply(1:10, R2))
# 使用mcsapply函数进行10次
system.time(mcsapply(1:10, R2))


# 使用sapply函数进行10000次
system.time(sapply(1:10000, R2))
# 使用mcsapply函数进行10000次
system.time(mcsapply(1:10000, R2))
```


实际上，发现当样本量比较小的时候，并行计算反而比不并行所需要的时间更久，因为涉及到分配等额外消耗。当样本量比较大的时候并行计算会快很多。


#### mcvapply()

由于R中并没有现成的parVapply函数，所以并行这一步的处理无法进行



# 第11次作业解答

## Question


Write an Rcpp function for Exercise 9.8 (page 278, Statistical Computing with R).

+ Compare the corresponding generated random numbers with pure R language using the function “qqplot”.
+ Campare the computation time of the two functions with the function “microbenchmark”.
+ Comments your results.


## Answer

This example appears in $[40]$. Consider the bivariate density
$$
f(x, y) \propto\left(\begin{array}{l}
n \\
x
\end{array}\right) y^{x+a-1}(1-y)^{n-x+b-1}, \quad x=0,1, \ldots, n, 0 \leq y \leq 1
$$
It can be shown (see e.g. [23]) that for fixed $a, b, n$, the conditional distributions are Binomial $(n, y)$ and Beta $(x+a, n-x+b)$. Use the Gibbs sampler to generate a chain with target joint density $f(x, y)$.


###  Gibbs in R

```{r}
a=1
b=1
N=10000
n=25

gibbsR=function(a,b,n,N){
  X=matrix(0, N, 2)  #样本阵
  X[1,]=c(0,0.5)
  for(i in 2:N){
    X2= X[i-1, 2]
    X[i,1]=rbinom(1,25,X2)
    X1=X[i,1]
    X[i,2]=rbeta(1,X1+a,25-X1+b)
  }
  return(X)
}

X=gibbsR(a,b,n,N)

plot(X[,1],X[,2],xlab = "x",ylab = "y",main = "gibbsR")
```

###  Gibbs in C

```{r,eval=FALSE}
library(Rcpp)
dir="D:/Software/Github/2021Fall/Statistical_Computing/Homework/hw10/"
sourceCpp(paste0(dir,"R.cpp"))
Xc=gibbsC(a,b,n,N)
plot(Xc[,1],Xc[,2],xlab = "x",ylab = "y",main="gibbsC")
```

### QQ plot

```{r, eval=FALSE}
qqplot(X[,1],Xc[,1],xlab = "gibbsR",ylab = "gibbsC",main="第1维变量QQ图")
abline(0,1,col = "red")

qqplot(X[,2],Xc[,2], xlab = "gibbsR",ylab = "gibbsC",main="第2维变量QQ图")
abline(0,1,col = "red")
```


> Rcpp和R产生的随机数的QQ图基本在一条直线上, 所以产生的随机数的分布基本一致。


### Time

```{r,eval=FALSE}
library(microbenchmark)
ts=microbenchmark(gibbR=gibbsR(a,b,n,N), gibbC=gibbsC(a,b,N,n))
summary(ts)[,c(1,3,5,6)]
```


> Rcpp运行的平均时间是20ms左右, 但R里运行的时间50000ms左右,Rcpp的效率远高于R

























